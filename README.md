\# Pipeline de Donn√©es IoT - Parc √âolien 



Ce projet impl√©mente une architecture distribu√©e √† 3 n≈ìuds pour la collecte, le nettoyage et l'analyse de donn√©es de capteurs √©oliens en temps r√©el.



\## üìÅ Structure du Projet

\- \*\*src/\*\* : Contient les scripts Python d'ingestion (Node 1) et d'archivage (Node 2).

\- \*\*data/\*\* : Scripts de simulation et g√©n√©ration de donn√©es brutes.

\- \*\*docs/\*\* : Rapport technique d√©taill√© et sch√©ma de l'architecture.

\- \*\*requirements.txt\*\* : D√©pendances n√©cessaires (Paho-MQTT, PyMongo, Redis).



\## üèó Architecture de la Solution

Le pipeline repose sur un d√©couplage des responsabilit√©s :

1\. \*\*MQTT (Mosquitto)\*\* : Transport des messages des turbines.

2\. \*\*Node 1 (Ingestion)\*\* : Nettoyage des donn√©es (imputation des `NaN` par la moyenne).

3\. \*\*Redis Streams\*\* : Syst√®me de buffer √† haute performance.

4\. \*\*Node 2 (Archivage)\*\* : Transfert persistant vers la base NoSQL.

5\. \*\*MongoDB\*\* : Stockage long terme et calcul des KPIs via moteur d'agr√©gation.



\## üõ† Installation et Utilisation

```bash

\# Installer les d√©pendances

pip install -r requirements.txt



\# Lancer les services Docker

docker run -d -p 6379:6379 redis

docker run -d -p 27017:27017 mongo



\# Lancer les composants

python src/node1\_ingestion.py #python3 ¬†src/node1\_ingestion.py

python src/node2\_archiver.py  #python3 ¬†src/node2\_archiver.py



\#Lancer les g√©n√©rateurs

python data/Turibne\_101\_Data\_Generator.py #python3 data/Turibne\_101\_Data\_Generator.py

python data/Turibne\_102\_Data\_Generator.py #python3 data/Turibne\_102\_Data\_Generator.py

python data/Turibne\_103\_Data\_Generator.py #python3 data/Turibne\_103\_Data\_Generator.py



